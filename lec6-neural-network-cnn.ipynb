{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n Winter 2016: Lecture 6\n",
    "## Topics\n",
    "- Neural Network\n",
    "\n",
    "**TODO:** add topics. hm maybe there is some plugin to do this quick?\n",
    "\n",
    "## Sources\n",
    "- video: https://www.youtube.com/watch?v=hd_KFJ5ktUc\n",
    "- original notes by Andrej Karpathy: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hd_KFJ5ktUc?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "video_id='hd_KFJ5ktUc'\n",
    "HTML(f'<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/{video_id}?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights update\n",
    "### SGD\n",
    "- problems\n",
    "  - very slow progress along flat directions and jitter along steep one\n",
    "  \n",
    "```python\n",
    "x -= learning_rate * dx\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum update\n",
    "```python\n",
    "v = mu * v - learning_rate * dx # integrated velociy\n",
    "x += v # intergrate position\n",
    "```\n",
    "- physical interpretation - ball rolling down the loss function + friction (`mu`)\n",
    "- allows a velocity to \"build up\" along shallow directions\n",
    "- velocity becomes damped in steep direction due to quickly changing sign\n",
    "- encourage process to the consistent direction\n",
    "- overshoot - because \"build up\" velocity\n",
    "- eventually converging down\n",
    "- usually `0.5 < mu < 0.99`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum\n",
    "- \"lookahead\" - get `dx` from point where we are going to be by momentum ($\\theta_{t-1} + \\mu u_{t-1}$)\n",
    "$$\n",
    "u_t = \\mu u_{t-1} - \\epsilon \\nabla f ( \\theta_{t-1} + \\mu u_{t-1})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} + u_t\n",
    "$$\n",
    "$\\epsilon$ - learning rate\n",
    "\n",
    "$\\mu$ - momentum coefficient\n",
    "\n",
    "- we use rearangment in common application. \n",
    "That it would look more like vanilla update\n",
    "\n",
    "lets use\n",
    "\n",
    "$$\n",
    "\\phi_{t-1} = \\theta_{t-1} + \\mu u_{t-1}\n",
    "$$\n",
    "\n",
    "thus\n",
    "\n",
    "$$\n",
    "u_t = \\mu u_{t-1} - \\epsilon \\nabla f (\\phi_{t-1})\n",
    "$$\n",
    "\n",
    "because\n",
    "\n",
    "$$\n",
    "\\theta_t = \\phi_t - \\mu u_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_t - \\mu u_t = \\phi_{t-1} - \\mu u_{t-1} + u_t\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\phi_t = \\phi_{t-1} - \\mu u_{t-1} + \\mu u_t + u_t\n",
    "$$\n",
    "\n",
    "we get:\n",
    "\n",
    "$$\n",
    "\\phi_t = \\phi_{t-1} - \\mu u_{t-1} + (1 + \\mu) u_t\n",
    "$$\n",
    "\n",
    "```python\n",
    "# x is \\phi here\n",
    "v_prev = v\n",
    "v = mu * v - learning_rate * dx\n",
    "x += -mu * v_prev + (1 + mu) * v\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad updatet\n",
    "```python\n",
    "cache += dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "- idea - element-wise scale by history sum of squares in each dimension\n",
    "- problems:\n",
    "  - decay of update afterwhile - thus it doesn't work well for very long time\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSProp update\n",
    "come from slides (lec6) of Geoff Hinton on Coursera\n",
    "```python\n",
    "cache += decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "x -= learning_rate * dx / (np.sqrt(cache) + 1e-7)\n",
    "```\n",
    "- slowly forgot previous cache what helps to keep moving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam update\n",
    "```python\n",
    "m = beta1 * m + (1 - beta1)*dx\n",
    "v = beta2 * v + (1 - beta2)*(dx**2)\n",
    "x -= learning_rate * m / (np.sqrt(v) + 1e-7)\n",
    "```\n",
    "\n",
    "- with bias correction \n",
    "needed because m, v initilize in a zero and incorrect in begining\n",
    "```python\n",
    "m,v = ...\n",
    "for t in range(1, big_number):\n",
    "  dx = # evalute gradient\n",
    "  m = beta1 * m + (1 - beta1)*dx\n",
    "  v = beta2 * v + (1 - beta2)*(dx**2)\n",
    "  \n",
    "  # bias correction (which works in very few steps)\n",
    "  mb = m / (1 - beta1 ** t) \n",
    "  mv = m / (1 - beta2 ** t)\n",
    "  \n",
    "  x -= learning_rate * mb / (np.sqrt(mv) + 1e-7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "**Should decay learning rate** because usually at the star of update we need moving fast but after while we need to slow down to converge to the minimum\n",
    "- step decay - _decay by half after few epochs_\n",
    "- exponential decay\n",
    "$$\n",
    "\\alpha = \\alpha_0 e^{-kt}\n",
    "$$\n",
    "- 1/t decay\n",
    "$$\n",
    "\\alpha = \\alpha_0 / ( 1 + kt)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order optimization methods\n",
    "[Newton's method in optimization](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization)\n",
    "$$\n",
    "J(\\theta) \\approx J(\\theta_0) + (\\theta - \\theta_0)^T \\nabla_\\theta J(\\theta_0) + \\frac{1}{2} (\\theta - \\theta_0)^T H(\\theta - \\theta_0)\n",
    "$$\n",
    "Newton parameter update\n",
    "$$\n",
    "\\theta^* = \\theta_0 - H^{-1} \\nabla_\\theta J(\\theta_0)\n",
    "$$\n",
    "$H$ - [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix)\n",
    "\n",
    "- nicer convergence\n",
    "- no hyperparameters\n",
    "- problem \n",
    "  - $H$ is squire matrix -- so we would have too many parameters `~1e6 x 1e6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BFGS\n",
    "\n",
    "Quasi-Newton method - instead of inverting the Hessian matrix ($O(n^3)$), aproximate inverse Hessian with rank 1 updates over time ($O(n^2)$ each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-BFGS\n",
    "Limited memory BFGS. Does not form/store the full inverse Hessian\n",
    "\n",
    "- pros \n",
    "  - usually works very well in full batch, deterministic mode\n",
    "- cons\n",
    "  - does not transfer very well to mini-batch settings\n",
    "  - doesn't work good with randomness so don't forget to disable all sources of noise\n",
    "  - too heavy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ensembles\n",
    "- almost always give you 2% extra performance\n",
    "### Related tricks\n",
    "- save checkpoints (on some epochs) from NN and make ensemble from them.\n",
    "- running average on `dx`\n",
    "```python\n",
    "while True:\n",
    "    data_batch = dataset.sample_data_batch()\n",
    "    loss = network.forward(data_batch)\n",
    "    dx = network.backward()\n",
    "    x += - learning * dx\n",
    "    x_test = 0.995 * x_test + 0.005 * x # use for test set. exponentially decaying of parameter x\n",
    "```\n",
    "almost always perform slightly better than `x` alone\n",
    "TODO: https://youtu.be/hd_KFJ5ktUc?list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&t=2182"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization. Dropout\n",
    "- forces the network to have a redundant representation\n",
    "- train a large ensable of models (that share parameters)\n",
    "  - subsampling of nn\n",
    "- _each binary mask is one model, gets trained on only ~one datapoint_\n",
    "- usually `50%`\n",
    "- in deep nn we usually start with small dropout and increase it on later layers\n",
    "- alternative: drop connect - instead of drop out neurons we drop out only connects\n",
    "- doesn't efficiant - _monte carlo approximation_ - do many forward passes with different dropouts masks, and avg all predictions\n",
    "- in test time \n",
    "  - we don't use dropout\n",
    "  - we should compensate fact that we drop out neurons on training time and mult activation to  `p` (`50%`)\n",
    "  - inverted dropout - actually we could mult activation to `1/p` on training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient checking\n",
    "TODO: get from offcial notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History of Convolutional Neural Network (CNN)\n",
    "- 1980 Fukushima. **Neurocongnitron**\n",
    "- 1998 LeCun, Botton, Bengio, Haffner. **LeNet-5**. Gradient-based learning applied to document recognition\n",
    "- 2012 Knizhevsky, Sutskever, Hinton. **AlexNet** + Relu. ImageNet Classification with Deep Convolution Neural. Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
