{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS231n Winter 2016: Lecture 4\n",
    "## Topics\n",
    "- Neural Network\n",
    "- Activation functions\n",
    "- Data Preprocessing\n",
    "- Weight Initialization\n",
    "- Batch normalization\n",
    "- Sanity check\n",
    "- Process Visualization\n",
    "\n",
    "## Sources\n",
    "- video: https://www.youtube.com/watch?v=gYpoJMlgyXA\n",
    "- original notes by Andrej Karpathy: http://cs231n.github.io/optimization-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/gYpoJMlgyXA?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 'gYpoJMlgyXA'\n",
    "from IPython.display import HTML\n",
    "HTML(f'<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/{id}?rel=0&amp;controls=1&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid activation function\n",
    "### Problems\n",
    "- it kills gradient for big values (saturated neurouns \"kills\" the gradients)\n",
    "- sigmoid outputs are not zero-centered\n",
    "  - example: when the input for a neuron (x) is always positive all coeficiens of W will be all positive or negative\n",
    "  $$\n",
    "  f(\\sum_i w_i x_i + b)\n",
    "  $$\n",
    "  and optimization will look like zig-zag path, slow convergence\n",
    "  TODO: I don't really get why is it\n",
    "- exp is bit compute exptensive\n",
    "\n",
    "## Relu\n",
    "- Solves almost all problems of Sigmoid. Except\n",
    "### Problems\n",
    "- Not zero-centered output\n",
    "- An annoyance - gradient of value less then 0 is 0.\n",
    "  - dead Relu - it could comes on initial state - when W define that way that it only produce 0 gradient on input data\n",
    "  - or it could become dead Relu after some iteration with high learning rate (step) optimization could set such high \n",
    "  values for W so it gets dead gradient and will never update. Andrej told that it possible to get 10% dead neurons \n",
    "  that way.\n",
    "  - decrease changes - init bies = 0.01 (slightly positive numbers) - controversial point\n",
    "  \n",
    "## LeakyRelu\n",
    "- Solve probem of `die relu`.\n",
    "\n",
    "## PPeLU (Parameteric Rectifier)\n",
    "$$\n",
    "f(x) = max(\\alpha x, x)\n",
    "$$\n",
    "and is learned $\\alpha$ by backprop\n",
    "\n",
    "## Exponential Linear Units (ELU)\n",
    "\n",
    "## Maxout Neuron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- zero-centering\n",
    "```\n",
    "X -= np.mean(X, axis=0)\n",
    "```\n",
    "- normalize\n",
    "```\n",
    "X /= np.std(X, axis=0)\n",
    "```\n",
    "- decorrelat (PCA) - data has diagonal covariance matrix\n",
    "- whitening - covariance matrix is identity matrix\n",
    "\n",
    "for images only common use zero-centering.\n",
    "- subtract the mean image [width, height, rgb]\n",
    "- subtract per-channel mean [r,g,b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight initialization\n",
    "\n",
    "### Shouldn't do\n",
    "- $W = 0$\n",
    "- small numbers\n",
    "```\n",
    "W = 0.001 * np.random.randn(D, H)\n",
    "```\n",
    "it works ok for small network but for big one you will get a problem: \n",
    "  - if we start with `mean = 0.0` and `std = 0.01` after each layer `mean` becomes more centered and `std -> 0`. and finally collapses to the `0`. Almost the same will be for gradient\n",
    "- std = 1.0\n",
    "```\n",
    "W = 1.0 * np.random.randn(D, H)\n",
    "```\n",
    "  - if we start with `mean = 0.0` and `std = 1.0` after few layers `W` will be saturated to `-1` and `1`. Gradients also be all `0`\n",
    "  \n",
    "### Should do\n",
    "- xavier initialization for `tanh`\n",
    "```\n",
    "W = np.random.randn(fan_in, fan_out) /np.sqrt(fan_in)\n",
    "```\n",
    "where `fan_in` - number of inputs\n",
    "  - it still decrease standard deviation but not so dramatic as previous\n",
    "  - it doesn't work for `ReLU` - decrese of standard deviation is much more rappid. Because `Relu` you 'kill' of distribution\n",
    "  \n",
    "- He et al. 2015\n",
    "```\n",
    "W = np.random.randn(fan_in, fan_out) /np.sqrt(fan_in / 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization\n",
    "just make unit gaussian activation\n",
    "$$\n",
    "\\hat{x}^{(k)} = \\frac{x^{(k)} - E[x^{(k)}]}{\\sqrt{Var[x^{(k)}]}}\n",
    "$$\n",
    "it works because it's completely (vanilla) differentiable function\n",
    "- for each feature independently\n",
    "- usage: `FC -> BN -> tanh -> FC -> BN -> tanh`. \n",
    "Because we are not sure that `tanh` would like gaussian on input. we shift and scale input:\n",
    "\n",
    "$$\n",
    "y^{(k)} = \\gamma^{(k)} \\hat{x}^{(k)} + \\beta^{(k)}\n",
    "$$.\n",
    "\n",
    "and learn thoese params. But it could learn to disable batch norm by getting:\n",
    "\n",
    "$$\n",
    "\\gamma^{(k)} = \\sqrt{Var[x^{(k)}]}\n",
    "$$\n",
    "$$\n",
    "\\beta^{(k)} = E[x^{(k)}]\n",
    "$$\n",
    "\n",
    "### Features\n",
    "- improves gradient flow through the network\n",
    "- allow higher learning rates\n",
    "- reduces the strong dependencies on initialization\n",
    "- act as a form of regularization in a funny way, and slightly reduces the need for dropout, maybe\n",
    "\n",
    "### Details\n",
    "- in test time mean/std are fixed and could be estimated during training with running averages - because we want more deterministic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check of nn\n",
    "## check the loss function\n",
    "\n",
    "\n",
    "- for 10 classes, with W init by `mean = 0` and `std = 0.0001` and `reg = 0.0` it should ~ log(10).\n",
    "- with `reg = 1e3` it should going up\n",
    "\n",
    "## try to overfit small pease data\n",
    "- 20 examples, 10 labels\n",
    "- turn off regularization `reg = 0`\n",
    "- use vanilla `sgd`\n",
    "- loss should be near `0`. and `100%` accuracy\n",
    "\n",
    "## Hyper parameter optimization\n",
    "- tune  learning rate\n",
    "  - sometime cost decrease slowly but in contrast accuraccy step much quicker. The reason that accuracy just takes into account bigger score, but accuracy (in softmax) works with all scores, so they could be near each other but we already have right leaders\n",
    "- coarse -> fine\n",
    "- grid search -> random layout\n",
    "  - because usually one parameter is better than others so when we use grid search we just get much less data samples on the most significant parameter then if we randomize input a little bit\n",
    "  \n",
    "## Visualize process\n",
    "- for example visulize all loss functions on cluster\n",
    "  - https://lossfunctions.tumblr.com/ loss function could tell a lot\n",
    "- accuracy\n",
    "- different between parameters and scale of your update of those parameters \n",
    "  - it should be about 1e-3. (`dW * learning rate / W`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
